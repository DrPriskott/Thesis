{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Processes - PyTorch implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B.:** pay attention to the tensors dimensions: a 3-by-1 vector can be represented as a tensor `[[1, 2, 3]]` or `[[1], [2], [3]]`. In the former case the shape is `tensor.Size([3])`, in the latter is `tensor.Size([3, 1])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you start with a dataframe as\n",
    "\\begin{bmatrix}\n",
    "    x_{1, 1} & \\cdots & x_{1, p} & y_{1} \\\\\n",
    "    x_{2, 1} & \\cdots & x_{2, p} & y_{2} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "    x_{n, 1} & \\cdots & x_{n, p} & y_{n}\\\\\n",
    "\\end{bmatrix}\n",
    "where you have $p$ features for each of the $n$ observations. Applying `h()` returns a $(n \\times \\text{r_dim})$ tensor with rows corresponding to `r_i` - in other words, for each observation $[x_{i,1}, \\dots, x_{i,p}], \\; i = 1, \\dots, n$, you obtain a vector $r_i = [r_{i,1}, \\dots, r_{i, \\text{r_dim}}], \\; i = 1, \\dots, n$\n",
    "\\begin{bmatrix}\n",
    "    r_{1,1} & \\cdots & r_{1, \\text{r_dim}} \\\\\n",
    "    r_{2,1} & \\cdots & r_{2, \\text{r_dim}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    r_{n,1} & \\cdots & r_{n, \\text{r_dim}} \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "Taking the mean along columns gives the `r` tensor with dimensions $1 \\times \\text{r_dim}$ \n",
    "\\begin{bmatrix}\n",
    "    \\bar{r}_{1} & \\cdots & \\bar{r}_{\\text{r_dim}} \\\\\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From context point data to r\n",
    "Suppose that $p = 1$, thus the input tensor and the `r_i` representations are, respectively,\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "    x_{1} & y_{1} \\\\\n",
    "    x_{2} & y_{2} \\\\\n",
    "    \\vdots & \\vdots \\\\\n",
    "    x_{n} & y_{n}\\\\\n",
    "\\end{bmatrix} & \\qquad\n",
    "\\begin{bmatrix}\n",
    "    r_{1, 1} & \\cdots & r_{1, \\text{r_dim}} \\\\\n",
    "    r_{2, 1} & \\cdots & r_{2, \\text{r_dim}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    r_{n, 1} & \\cdots & r_{n, \\text{r_dim}} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From r to z_mean, z_logvar\n",
    "Then `r` is mapped to the mean and variace of `z` to produce two vectors `z_mean` and `z_logvar`, respectively,\n",
    "\\begin{align*}\n",
    "\\text{z_mean} = \\begin{bmatrix}\n",
    "    z_{\\text{mean}, 1} & \\cdots & z_{\\text{mean, z_dim}}\n",
    "\\end{bmatrix} & \\qquad \n",
    "\\text{z_logvar} =\n",
    "\\begin{bmatrix}\n",
    "    z_{\\text{logvar}, 1} & \\cdots & z_{\\text{logvar, z_dim}}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "That parametrize the distribution of `z` as follows \n",
    "$$z \\sim \\mathcal{N}\\left(\\text{z_mean}, \\; I_{\\text{z_dim}} \\times \\exp\\left(\\frac{\\text{z_logvar}}{2}\\right) \\right)$$\n",
    "where $I_{\\text{z_dim}}$ is a $\\text{z_dim}$-dimensional diagonal matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a sample\n",
    "\n",
    "using the reparametrization trick\n",
    "$$z = \\text{z_mean} + \\frac{\\text{z_logvar}}{2} \\times \\varepsilon, \\qquad \\varepsilon \\sim \\mathcal{N}\\left(0, I_{\\text{z_dim}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prediction y_target\n",
    "As soon as `x_target` is available, use it together with the sampled `z` to predict `y_target`. Suppose we have $m$ target points, i.e. $x_t, \\; t=1,\\dots,m$. The input of `g()` are\n",
    "\n",
    "\\begin{bmatrix}\n",
    "    x_{1,1} & z_{1} & \\cdots & z_{\\text{z_dim}} \\\\\n",
    "    x_{2,1} & z_{1} & \\cdots & z_{\\text{z_dim}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    x_{m,1} & z_{1} & \\cdots & z_{\\text{z_dim}} \\\\\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x \t\t y \n",
      " [[-2.         -0.9092974 ]\n",
      " [-1.         -0.84147096]\n",
      " [ 0.          0.        ]\n",
      " [ 1.          0.84147096]\n",
      " [ 2.          0.9092974 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_x_np = np.arange(-2, 3, 1).reshape(-1, 1).astype(np.float32)\n",
    "all_y_np = np.sin(all_x_np)\n",
    "print(' x \\t\\t y \\n', np.concatenate((all_x_np, all_y_np), axis=1))\n",
    "\n",
    "plt.scatter(all_x_np,all_y_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h_specs and g_specs must be list of tuples like `[(output_dim_1, torch.nn.ReLU()), ..., (output_dim_H, activation_H()), (r_dim, None)]` for `H` hidden layers and linear (no activations) output layer with dimension `r_dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NP(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_labels, z_dim, h_specs, g_specs):\n",
    "        \"\"\"Defines the attributes, i.e. the modules that define the \n",
    "           operation needed to implement the Neural Process\n",
    "           \n",
    "            Inputs:\n",
    "                n_features: number of features per observation, i.e. number of \n",
    "                            components of vector `x_i`, i = 1, ..., n\n",
    "                \n",
    "                n_labels: number of components of the output vector `y_i`\n",
    "                            i = 1, ..., n\n",
    "                \n",
    "                z_dim: dimension of the latent `z` vector\n",
    "                \n",
    "                h_specs: list of tuples defining the number of units and activation\n",
    "                            functions per each layer of the encoder. The size of the \n",
    "                            `r` representation must be passed as last tuple. If no \n",
    "                            activation is needed put `None`; \n",
    "                            e.g. two layers with 8 hidden units and relu activation \n",
    "                            function, output layer is linear:\n",
    "                                [(8, torch.nn.ReLU()), (8, torch.nn.ReLU()), (r_dim, None)]\n",
    "                \n",
    "                g_specs: list of tuples defining the number of units and activation\n",
    "                            functions per each layer of the decoder. The size of the \n",
    "                            If no activation is needed put `None`; The size of the \n",
    "                            output layer, i.e. the number of components of the `y_target` \n",
    "                            must agree with the dimension of `y_context`\n",
    "                            e.g. two layers with 8 hidden units and relu activation \n",
    "                            function, output layer is linear:\n",
    "                                [(8, torch.nn.ReLU()), (8, torch.nn.ReLU()), (y_context_dim, None)]\n",
    "                                \n",
    "            \n",
    "            Returns:\n",
    "                y_hat: the predicted value for `y_target` based on `z` and `x_target`\n",
    "                \n",
    "               \n",
    "           \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_features = n_features \n",
    "        self.n_labels = n_labels\n",
    "        self.z_dim = z_dim #number of components of z\n",
    "        self.h_specs = h_specs\n",
    "        self.g_specs = g_specs\n",
    "        self.h() #call the h() method in the constructor to modules for the layers\n",
    "        self.r_to_z() #call the r_to_z() method in the constructor to add module that maps r -> (z_mu, z_logvar)  \n",
    "        self.g() #call the g() method in the constructor to modules for the layers\n",
    "            \n",
    "    def h(self):\n",
    "        \"\"\"Function that dynamically creates attributes for \n",
    "           the encoder network `h()`; needs to be run in the \n",
    "           constructor; check that every layer is defined by\n",
    "           accessing the OrderedDict in object._modules\"\"\"\n",
    "        h_input_dim = self.n_features + self.n_labels #number of features + output per observation\n",
    "        \n",
    "        for i in range(len(self.h_specs)):\n",
    "            if i == 0:    \n",
    "                setattr(self, 'h_layer' + str(i), torch.nn.Linear(h_input_dim, self.h_specs[i][0]))\n",
    "                if self.h_specs[i][1]:\n",
    "                    setattr(self, 'h_layer' + str(i) + '_act', self.h_specs[i][1])\n",
    "            else:\n",
    "                setattr(self, 'h_layer' + str(i), torch.nn.Linear(self.h_specs[i-1][0], self.h_specs[i][0]))\n",
    "                if self.h_specs[i][1]:\n",
    "                    setattr(self, 'h_layer' + str(i) + '_act', self.h_specs[i][1])  \n",
    "        return\n",
    "                   \n",
    "                \n",
    "    def aggregate(self, x):\n",
    "        \"\"\"Computes the mean of the elements in the r_i \n",
    "           tensor created by `h()`; not called in the \n",
    "           constructor, it is simply called in the `forward()`\n",
    "           method; it takes the mean along columns of the tensor\"\"\"\n",
    "        return torch.mean(x, dim=0)\n",
    "    \n",
    "    \n",
    "    def r_to_z(self):\n",
    "        \"\"\"Function that creates attributes in for the \n",
    "           mapping function r -> (z_mean, z_logvar)\"\"\"\n",
    "        setattr(self, 'r_to_mean', torch.nn.Linear(self.h_specs[-1][0], self.z_dim)) # self.h_specs[-1][] since r_dim is in the last tuple\n",
    "        setattr(self, 'r_to_logvar', torch.nn.Linear(self.h_specs[-1][0], self.z_dim))\n",
    "        setattr(self, 'r_to_logvar_softplus', torch.nn.Softplus())\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def sample_z(self, z_mean, z_logvar):\n",
    "        \"\"\"Function used to sample from z using the \n",
    "           reparametrization \n",
    "               z = z_mean + exp(1/2 * z_logvar) * eps\n",
    "           where eps ~ N(0, I)\"\"\"\n",
    "        var = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(var) \n",
    "        z_sample = eps.mul(var).add_(z_mean)\n",
    "        z_sample = z_sample.unsqueeze(-1) # from [[1,2,3]] to [[1], [2], [3]]\n",
    "        return z_sample\n",
    "\n",
    "    \n",
    "    def g(self):\n",
    "        \"\"\"Function that dynamically creates attrbutes for \n",
    "           the decoder network `g()`; needs to be run in the \n",
    "           constructor; check that every layer is defined by\n",
    "           accessing the OrderedDict in object._modules\"\"\"\n",
    "        g_n_features = self.n_features + self.z_dim #number of features\n",
    "        \n",
    "        for i in range(len(self.g_specs)):\n",
    "            if i == 0:    \n",
    "                setattr(self, 'g_layer' + str(i), torch.nn.Linear(g_n_features, self.g_specs[i][0]))\n",
    "                if self.g_specs[i][1]:\n",
    "                    setattr(self, 'g_layer' + str(i) + '_act', self.g_specs[i][1])\n",
    "            else:\n",
    "                setattr(self, 'g_layer' + str(i), torch.nn.Linear(self.g_specs[i-1][0], self.g_specs[i][0]))\n",
    "                if self.g_specs[i][1]:\n",
    "                    setattr(self, 'g_layer' + str(i) + '_act', self.g_specs[i][1])    \n",
    "                \n",
    "    \n",
    "    def forward(self, xy_context, x_target=None):\n",
    "        \"\"\"Opearations to be implemented at each forward pass;\n",
    "           the modules to be used are already build in the \n",
    "           constructor, just call them accessing the `_modules`\n",
    "           attributes\"\"\"\n",
    "\n",
    "        # from context data to r_i\n",
    "        r_i = xy_context\n",
    "        for layer_name, layer_func in self._modules.items():\n",
    "            if layer_name.startswith('h'):\n",
    "                r_i = layer_func(r_i)\n",
    "                \n",
    "        # from r_i's to r\n",
    "        r = self.aggregate(r_i)\n",
    "        \n",
    "        # from r to z_mean and z_logvar\n",
    "        z_mean = self._modules['r_to_mean'](r)\n",
    "        z_logvar = self._modules['r_to_logvar_softplus'](self._modules['r_to_logvar'](r))\n",
    "        \n",
    "        # sample z from the distribution\n",
    "        z_sample = self.sample_z(z_mean, z_logvar)\n",
    "        \n",
    "        # concatenate x_target and z\n",
    "        xz = torch.cat([x_target, z_sample.view(z_sample.shape[1], z_sample.shape[0]).expand(x_target.shape[0], -1)], dim=1)\n",
    "        y_target = xz\n",
    "        for layer_name, layer_func in self._modules.items():\n",
    "            if layer_name.startswith('g'):\n",
    "                y_target = layer_func(y_target)\n",
    "        \n",
    "        return y_target, z_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to split randomly into context and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split_context_target(x, y, n_context):\n",
    "    \"\"\"splits and returns tensors\"\"\"\n",
    "    x_index = np.arange(x.shape[0])\n",
    "    mask = np.random.choice(x_index, size=n_context, replace=False)\n",
    "    x_context_np = x[mask]\n",
    "    y_context_np = y[mask]\n",
    "    x_target_np = np.delete(x, mask, axis=0) \n",
    "    y_target_np = np.delete(y, mask, axis=0)\n",
    "    \n",
    "    x_context_tensor = torch.from_numpy(x_context_np)\n",
    "    y_context_tensor = torch.from_numpy(y_context_np)\n",
    "    x_target_tensor = torch.from_numpy(x_target_np)\n",
    "    y_target_tensor = torch.from_numpy(y_target_np)\n",
    "    \n",
    "    return x_context_tensor, y_context_tensor, x_target_tensor, y_target_tensor\n",
    "\n",
    "\n",
    "def visualise(x, y, x_star):\n",
    "    z_mu, z_std = data_to_z_params(x,y)\n",
    "    zsamples = sample_z(z_mu, z_std, 100)\n",
    "    \n",
    "    mu, _ = decoder(x_star, zsamples)\n",
    "    for i in range(mu.shape[1]):\n",
    "        plt.plot(x_star.data.numpy(), mu[:,i].data.numpy(), linewidth=1)\n",
    "    plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_context = 10\n",
    "x = np.arange(-5,3, 0.1).reshape(-1,1).astype(np.float32)\n",
    "y = np.sin(x)\n",
    "x_c, y_c, x_t, y_t = random_split_context_target(x, y, n_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_dim = 3\n",
    "z_dim = 7\n",
    "h_specs = [(400, torch.nn.ReLU), (50, torch.nn.ReLU), (r_dim, None)]\n",
    "g_specs = [(55, torch.nn.ReLU), (60, torch.nn.ReLU), (x_t.shape[1], None)]\n",
    "\n",
    "model = NP(n_features=x_c.shape[1], n_labels=y_c.shape[1], h_specs=h_specs, z_dim=4, g_specs=g_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat, z_sample = model(torch.cat([x_c, y_c], dim=1), x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD9CAYAAACvMECaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAHMdJREFUeJzt3X+wXHWZ5/H3x0vEDGuRBBIIN0RSZRYW+RFm7sax2JmigBBUhkREDOWy11IqsjOUqyLLRUqZxaGMm2Fx3VV3Ij8mYyG/FCEDSiYEKSlKkQskEgZisqBykwwEQxBrskjCs3/0t+Gk6b59uk/f/nH786rq6j6nv+f0I0KePOf7Pc9RRGBmZlbP2zodgJmZ9QYnDDMzy8UJw8zMcnHCMDOzXJwwzMwsFycMMzPLpSUJQ9KZkjZL2ipppMr3B0q6NX3/sKSj0v5Fkh6V9ER6PzVzzAPpnBvSa1YrYjUzs+YcUPQEkgaAbwCLgDHgEUlrIuKfM8M+CbwUEe+WtAz4KvBR4EXgLyJiu6TjgLXAYOa4j0XEaNEYzcysuFZUGAuBrRHxTET8AbgFWFIxZgmwOn3+HnCaJEXE4xGxPe1/EniHpANbEJOZmbVYKxLGIPBcZnuM/auE/cZExF7gZeCQijEfBh6PiFcz+25Ml6O+KEktiNXMzJpU+JIUUO0P8sp+I+OOkfQeSpepzsh8/7GI2CbpncD3gQuAf3jLj0vLgeUABx100J8cc8wxjUVvZtbnHn300RcjYma9ca1IGGPAkZntOcD2GmPGJB0AHAzsApA0B/gB8J8i4v+WD4iIben9FUnfpXTp6y0JIyJWAasAhoaGYnTUUx5mZo2Q9Os841pxSeoRYL6keZLeDiwD1lSMWQMMp8/nAvdHREiaBtwDXB4RD5UHSzpA0qHp8xTgLGBTC2I1M7MmFU4YaU7iYkornJ4CbouIJyVdJensNOx64BBJW4HPAeWltxcD7wa+WLF89kBgraRfABuAbcC3i8ZqZmbN02Rqb+5LUmZmjZP0aEQM1RvnO73NzCwXJwwzM8vFCcPMzHJxwjAzs1ycMMzMLBcnDDMzy8UJw8zMcnHCMDOzXJwwzMwsFycMMzPLxQnDzMxyccIwM7NcnDDMzCwXJwwzM8vFCcPMzHJxwjAzs1xa8Uzvnnfn49tYuXYz23fv4YhpU7l08dEsPWmw02GZmXWVllUYks6UtFnSVkkjVb4/UNKt6fuHJR2V+e7ytH+zpMV5z9kKdz6+jcvveIJtu/cQwLbde7j8jie48/FtE/FzZmY9qyUJQ9IA8A3g/cCxwPmSjq0Y9kngpYh4N3At8NV07LHAMuA9wJnANyUN5DxnYSvXbmbPa/v227fntX1ccttG5o3cw8kr7nfyMDOjdRXGQmBrRDwTEX8AbgGWVIxZAqxOn78HnCZJaf8tEfFqRDwLbE3ny3POwrbv3lN1/74IVxxmZhmtShiDwHOZ7bG0r+qYiNgLvAwcMs6xec6JpOWSRiWN7ty5s+HAj5g2te6YPa/tY+XazQ2f28xsMmlVwlCVfZFzTKP7998RsSoihiJiaObMmXUDrXTp4qOZOmWg7rhtu/f48pSZ9bVWrZIaA47MbM8BttcYMybpAOBgYFedY+uds7DyaqjyKqm3SeyLt+Ql4M3LU9njzMz6RasqjEeA+ZLmSXo7pUnsNRVj1gDD6fO5wP0REWn/srSKah4wH/h5znO2xNKTBnlo5FSeXfFBrjnvxHErDl+eMrN+1ZIKIyL2SroYWAsMADdExJOSrgJGI2INcD3wHUlbKVUWy9KxT0q6DfhnYC/wVxGxD6DaOVsR73iyFce2GhPitSbKzcwmM0WNyy+9aGhoKEZHR1t2vpNX3F81aQxOm8pDI6cCvunPzHqfpEcjYqjeOLcGGUe1CfGpUwa4dPHRgG/6M7P+4gqjjvEqiFoVyIDE6xGuOMysJ+StMNxLqo6lJw3W/AN/vJv+wKuqzGxy8SWpAvLe9Oc2I2Y2GThhFJD3pj+3GTGzycBzGAVl5zjGu+kvy3McZtZNPIfRJtk5jvKqqcrut5U8x2FmvcgVRou54jCzXuMKo0NccZjZZOVJ7wm09KRBvnLO8QxOm4ooVRL1uFeVmXUrVxgTrJmKw72qzKwbOWG0Ud5W6nnu7zAzazcnjDarV3FU9qpyY0Mz6xZOGB1UWXFkk0JlMvGEuJl1mpfVdik3NjSzdvGy2h7nxoZm1m0KLauVNEPSOklb0vv0GuOG05gtkobTvj+SdI+kpyU9KWlFZvzHJe2UtCG9LiwSZy9yY0Mz6zZF78MYAdZHxHxgfdrej6QZwJXAe4GFwJWZxPK3EXEMcBJwsqT3Zw69NSIWpNd1BePsOW5saGbdpmjCWAKsTp9XA0urjFkMrIuIXRHxErAOODMi/jUifgwQEX8AHgPmFIxn0mj2pj9XHGY2UYrOYRwWETsAImKHpFlVxgwCz2W2x9K+N0iaBvwF8D8zuz8s6c+BXwKfjYjsOfqC24yYWTepW2FIuk/SpiqvJTl/o9pfjd9YmiXpAOBm4OsR8Uza/Y/AURFxAnAfb1Yx1eJbLmlU0ujOnTtzhtR7XHGYWacVWlYraTNwSqouZgMPRMTRFWPOT2M+lbb/Lo27OW3fAPw+Ij5d4zcGgF0RcXC9eCbTstp68lYcWVOnDPCVc453xWFm+8m7rLboHMYaYDh9HgbuqjJmLXCGpOlpsvuMtA9JfwMcDHwme0BKPmVnA08VjHPSabbi+MytG1xtmFlTilYYhwC3AXOB3wAfiYhdkoaAiyLiwjTuE8AX0mFXR8SNkuZQmtt4Gng1ffe/I+I6SV+hlCj2AruA/xwRT9eLp58qjEqNVhyuNsysLG+F4Tu9J5FGH940OG0qD42c2qbozKxbOWH0ubwVh8BtRsz6nFuD9LlsY8NqPanKsjf9ZY8zM6vkCqMP5K023NjQrD+5wrA3VLZRr/VXBN/0Z2bjcYXRh2q1Tq/kisOsP7TrPgzrQW5saGbNcIXRpxpdgguuOMwmK89h2Ljc2NDMGuUKwwBXHGb9zBWGNcQVh5nV40lvews3NjSzalxhWFXNVBzgasNsMnOFYXU1WnHseW0fK9dubk9wZtY2rjAsl0Yrju05bgw0s97ihGENy9PY8IhpU9sZkpm1gROGNaVccVSrNqZOGeDSxW8+qTe7ZNdLcM16V+E5DEkzJK2TtCW9T68xbjiN2SJpOLP/AUmbJW1Ir1lp/4GSbpW0VdLDko4qGqu1XuX8xuC0qfs9ya+cULalpoduM2LWuwrfuCfpvwO7ImKFpBFgekRcVjFmBjAKDFF6BMOjwJ9ExEuSHgA+HxGjFcf8JXBCRFwkaRnwoYj46Hix+Ma97lOr0aFv+jPrHu1sPrgEWJ0+rwaWVhmzGFgXEbsi4iVgHXBmA+f9HnCalOOGAOsqtSa/3djQrPe0ImEcFhE7ANL7rCpjBoHnMttjaV/Zjely1BczSeGNYyJiL/AycEgL4rU2yjP5vee1fVxy20bmjdzjG//MuliuhCHpPkmbqryW5PydapVB+VrYxyLieODP0uuCHMdkY1suaVTS6M6dO3OGY+3iVupmk0euhBERp0fEcVVedwHPS5oNkN5fqHKKMeDIzPYcYHs697b0/grwXWBh5TGSDgAOBnZViW1VRAxFxNDMmTPz/M+xNmq2zYgrDrPu04pLUmuA8qqnYeCuKmPWAmdImp5WUZ0BrJV0gKRDASRNAc4CNlU577nA/TGZWuv2kaUnDfLQyKk8u+KDXHPeia44zHpUK1ZJHQLcBswFfgN8JCJ2SRoCLoqIC9O4TwBfSIddHRE3SjoI+AkwBRgA7gM+FxH7JL0D+A5wEqXKYllEPDNeLF4l1RvcSt2su+RdJeXnYVhHNdLYsGzqlIH97vUws2L8TG/rCW6lbtY73BrEOs6t1M16gysM6ypupW7WvTyHYV0tb8Uh8IS4WZP8TG+bFPK0Ugf2W4KbPc7MWscVhvWMvNWGl+CaNcYVhk062Wpje2qXXk35vg5XHGat5QrDelat1umVXHGYjc/3Ydik58aGZu3lCsN6mtuMmBXnOQzrC83c9Oc5DrPm+JKUTRrNthnxjX9m+bjCsEmlmYqj1mNkzWx/Thg2aVUuw601x1F+jGx2PsTzG2Zv5Ulv6xvVKo5yq3Sg5ndOGjbZ+XkYZlXUqiJq3dPhFVXWD7xKyqyK7BxHVq15DK+oMntToVVSkmZIWidpS3qfXmPccBqzRdJw2vdOSRsyrxclfS1993FJOzPfXVgkTrN6yvMY4/GKKut3RZfVjgDrI2I+sD5t70fSDOBK4L3AQuBKSdMj4pWIWFB+Ab8G7sgcemvm++sKxmk2rrx3jXtFlfWzogljCbA6fV4NLK0yZjGwLiJ2RcRLwDrgzOwASfOBWcCDBeMxa0reezjyVCJmk1XROYzDImIHQETskDSryphB4LnM9ljal3U+pYoiOwP/YUl/DvwS+GxEPEcVkpYDywHmzp3b3P8KM+rfwzF1ygCXLj76jW0vw7V+UzdhSLoPOLzKV1fk/I1qf1WrXJq1DLggs/2PwM0R8aqkiyhVL6dWO3lErAJWQWmVVM6YzMZVeQ9HZUKoTCieFLd+UGhZraTNwCmpupgNPBARR1eMOT+N+VTa/rs07ua0fSJwe0T82xq/MQDsioiD68XjZbXWLl6Ga5NJu9qbrwGG0+dh4K4qY9YCZ0ianlZRnZH2lZ0P3Jw9ICWfsrOBpwrGadZS4y3DdSt1m6yKJowVwCJJW4BFaRtJQ5KuA4iIXcCXgUfS66q0r+w8KhIG8GlJT0raCHwa+HjBOM1aKu8y3Etu28i8kXs4ecX9Th7W83ynt1kT8jY2zHKrEetWbg1iNsH88CabLNwaxGyC+eFN1m/8ACWzFmj24U2fuXWD5zesZ7jCMGuRZioOcLVhvcNzGGYTpNE5Ds9vWKd4DsOswxqtODy/Yd3OcxhmbZCd48jDrdStG/mSlFmbNTK/IfAlKptwviRl1qUqGxuON7+RbTOSPdasE1xhmHVY3orDk+I2UVxhmPWIyoqj1l/hPCluneYKw6zL1GqdXskVh7VKu9qbm1mL5X2+uFupW7u5wjDrQm5saO3kOQyzHubGhtaNfEnKrMu5saF1i8IJQ9IMSeskbUnv02uMu1fSbkl3V+yfJ+nhdPytkt6e9h+Ytrem748qGqtZr1p60iAPjZzKsys+yDXnnZhrjgM8v2Gt1YoKYwRYHxHzgfVpu5qVwAVV9n8VuDYd/xLwybT/k8BLEfFu4No0zqzvNVpx+FGx1iqFJ70lbQZOiYgdkmYDD0TE0TXGngJ8PiLOStsCdgKHR8ReSe8D/joiFktamz7/VNIBwL8AM2OcgD3pbf2o0cfF+lGxVqmdy2oPi4gdAOl9VgPHHgLsjoi9aXsMKP9bPAg8l867F3g5jTezjGYaG7risGbkWiUl6T7g8CpfXVHw96vV0pHjuzdPIC0HlgPMnTu3YDhmvam8qsorqmwi5UoYEXF6re8kPS9pduaS1AsN/P6LwDRJB6QqYg6wPX03BhwJjKVLUgcDu6rEtgpYBaVLUg38ttmk00hjw7JyK3UnDKunFZek1gDD6fMwcFfeA9N8xI+Bc6scnz3vucD9481fmFlJMyuqtu3e40tUVlcrEsYKYJGkLcCitI2kIUnXlQdJehC4HThN0pikxemry4DPSdpKaY7i+rT/euCQtP9z1F59ZWY1NLKiym1GrB63BjHrI26lbtW4NYiZvYVbqVsRrjDM+phbqRu4vbmZ5eBW6tYIJwyzPubGhtYIz2GY9blmWqmD5zf6kecwzGw/jT68yfMbvc+rpMysKY1WHF5R1T88h2FmNTXT2HDl2s0THJV1ihOGmY2r3Grkax9dkGtF1fYcy3StN/mSlJnlkrex4RE5qxHrPU4YZpZbvfmNqVMGuHTxm89Py06ge1K89zlhmFlTKiuOyoRQmVA8Kd77nDDMrGnZiqPSyrWb37K6ys/e6G2e9DazCVFr8tuT4r3LFYaZTYgjpk2t2tgwOynuOY7e4grDzCZEtcaG2Unx8hzHttRm3Y0Nu1+hhCFphqR1krak9+k1xt0rabekuyv23yRps6RNkm6QNCXtP0XSy5I2pNeXisRpZu1X2dhwcNpUvnLO8ftNlleb47jkto1+XGyXKnpJagRYHxErJI2k7cuqjFsJ/BHwqYr9NwH/MX3+LnAh8K20/WBEnFUwPjProPEmxWvNZbjVSPcqeklqCbA6fV4NLK02KCLWA69U2f/DSICfA3MKxmNmPSLPDX5upd5diiaMwyJiB0B6n9XMSdKlqAuAezO73ydpo6QfSXpPwTjNrMvkfXgTeH6jW9S9JCXpPuDwKl9d0cI4vgn8JCIeTNuPAe+KiN9L+gBwJzC/RnzLgeUAc+fObWFIZjaR8rYaKSvPb3z21g1eUdUhhZ6HIWkzcEpE7JA0G3ggIo6uMfYU4POV8xKSrgROAs6JiNdrHPsrYCgiXhwvHj8Pw6x3NfLwJiituMpOolvz2vVM7zXAcPo8DNzVyMGSLgQWA+dnk4Wkw6XSsyIlLUxx/rZgrGbWxZpppe4VVe1VNGGsABZJ2gIsSttIGpJ0XXmQpAeB24HTJI1JWpy++j/AYcBPK5bPngtskrQR+DqwLCbTowHNrKpGW6nvi/A9HG3kR7SaWVdq9FGxULrX46GRU9sQ3eSS95KUE4aZdb1G5jcEnhRvkJ/pbWaTRiMrqrKXqLLHWnGuMMys5+StOAYkXo9wxVGHKwwzm7QqK45af+11m5HWcoVhZj3v5BX3V22lXskVR3Xtug/DzKzj8rYZ8TLcYpwwzKznVbZSHyjd9zsuNzZsnOcwzGxSyLZSb2QZruc38vMchplNSo3e+NfP8xteJWVmfa3RisMrqupzhWFmfaFcceRZTQX9VXF4lZSZWYYbGxbnCsPM+k4zjQ0nc8XhOQwzsxqaWVHlOQ5fkjKzPtfsPRwr126e+OC6jC9JmZll9GMr9bZMekuaIWmdpC3pfXqNcfdK2i3p7or9fy/p2fS0vQ2SFqT9kvR1SVsl/ULSHxeJ08wsr0Yqjn6bFC96SWoEWB8R84H1abualcAFNb67NCIWpNeGtO/9wPz0Wg58q2CcZma5lVdUPbvig1xz3ol1V1X1yyWqogljCbA6fV4NLK02KCLWA680eN5/iJKfAdMkzS4UqZlZEyorjlq257y/o5cVXSV1WETsAIiIHZJmNXGOqyV9iVShRMSrwCDwXGbMWNq3o2C8ZmYNy66qqtVK/YhpU9/4nF22OxnmOMrqVhiS7pO0qcprSQt+/3LgGODfAzOAy8o/W2Vs1dl5ScsljUoa3blzZwtCMjOrrVor9alTBrh08dHAm5Pm29KDnSbTHEfdhBERp0fEcVVedwHPly8VpfcXGvnxiNiRLju9CtwILExfjQFHZobOAbbXOMeqiBiKiKGZM2c28vNmZg2rvEQ1OG0qXznn+P2eAli5wmqytFIveklqDTAMrEjvdzVysKTZ6VKWKM1/bMqc92JJtwDvBV4uX/oyM+u07CWqSuPNZfT6TX9FJ71XAIskbQEWpW0kDUm6rjxI0oPA7cBpksYkLU5f3STpCeAJ4FDgb9L+HwLPAFuBbwN/WTBOM7O2yM5lVLPntX1ccttG5o3c03MVh2/cMzNroUZu/IPS/Ef2klYnuFutmVkHZOc48uilezicMMzMWqzRVuq9cg+Hu9WamU2Q7Mqp8Vqp98o9HE4YZmYTqF4r9Wr3cJS/77ZVVb4kZWbWJs3ew9EtcxyuMMzM2qiZezi27d7DvJF7On6JyhWGmVmXGO8ejm5oM+KEYWbWJar1qarUyUtUviRlZtYlKldV1bqtulPLcJ0wzMy6SCOt1Nu9BNeXpMzMutR4rdQ70UbdCcPMrEuNtwy3E0twfUnKzKyL1VqGW2seYyLnN1xhmJn1oFpLcOu1Vy/CCcPMrAfVe1TsRPAlKTOzHlS5BLcdq6ScMMzMetR4bUYmQqFLUpJmSFonaUt6n15j3L2Sdku6u2L/g5I2pNd2SXem/adIejnz3ZeKxGlmZsUVncMYAdZHxHxgfdquZiVwQeXOiPiziFgQEQuAnwJ3ZL5+sPxdRFxVME4zMyuoaMJYAqxOn1cDS6sNioj1wCu1TiLpncCpwJ0F4zEzswlSNGEcFhE7ANL7rCbP8yFKlcrvMvveJ2mjpB9Jek/BOM3MrKC6k96S7gMOr/LVFS2M43zgusz2Y8C7IuL3kj5AqfKYXyO+5cBygLlz57YwJDMzy6qbMCLi9FrfSXpe0uyI2CFpNvBCowFIOgRYSKnKKP/m7zKffyjpm5IOjYgXq8S3ClgFMDQ0VKu5o5mZFVT0ktQaYDh9HgbuauIcHwHujoj/V94h6XBJSp8Xpjh/WzBWMzMroGjCWAEskrQFWJS2kTQk6Y1LTJIeBG4HTpM0Jmlx5hzLgJsrznsusEnSRuDrwLKIcPVgZtZBmkx/Dg8NDcXo6GinwzAz6ymSHo2IoXrj3EvKzMxyccIwM7NcnDDMzCwXJwwzM8vFCcPMzHJxwjAzs1ycMMzMLBcnDDMzy8UJw8zMcnHCMDOzXCZVaxBJO4FfFzjFocBbOuJ2gW6NC7o3tm6NC7o3NsfVuG6NrdG43hURM+sNmlQJoyhJo3n6qbRbt8YF3Rtbt8YF3Rub42pct8Y2UXH5kpSZmeXihGFmZrk4YexvVacDqKFb44Luja1b44Lujc1xNa5bY5uQuDyHYWZmubjCMDOzXJwwMiT9taRtkjak1wc6HVMlSZ+XFJIO7XQsAJK+LOkX6Z/XP0k6otMxlUlaKenpFN8PJE3rdEwAkj4i6UlJr0vq+AobSWdK2ixpq6SRTsdTJukGSS9I2tTpWLIkHSnpx5KeSv8//pdOx1Qm6R2Sfi5pY4rtv7Xy/E4Yb3VtRCxIrx92OpgsSUdSenb6bzodS8bKiDghIhYAdwNf6nRAGeuA4yLiBOCXwOUdjqdsE3AO8JNOByJpAPgG8H7gWOB8Scd2Nqo3/D1wZqeDqGIvcElE/DvgT4G/6qJ/Zq8Cp0bEicAC4ExJf9qqkzth9JZrgf8KdM3EU0T8LrN5EN0V2z9FxN60+TNgTifjKYuIpyJic6fjSBYCWyPimYj4A3ALsKTDMQEQET8BdnU6jkoRsSMiHkufXwGeAgY7G1VJlPw+bU5Jr5b9N+mE8VYXp0sYN0ia3ulgyiSdDWyLiI2djqWSpKslPQd8jO6qMLI+Afyo00F0oUHgucz2GF3yh18vkHQUcBLwcGcjeZOkAUkbgBeAdRHRstgOaNWJeoWk+4DDq3x1BfAt4MuUMvKXgWso/UHTDbF9ATijXbFkjRdXRNwVEVcAV0i6HLgYuLJbYktjrqB0GeGmboqrS6jKvq6pEruZpH8DfB/4TEWl3VERsQ9YkObsfiDpuIhoyTxQ3yWMiDg9zzhJ36Z0Tb5tasUm6XhgHrBREpQurTwmaWFE/Eun4qriu8A9tDFh1ItN0jBwFnBatHENeQP/zDptDDgysz0H2N6hWHqGpCmUksVNEXFHp+OpJiJ2S3qA0jxQSxKGL0llSJqd2fwQLfqHXFREPBERsyLiqIg4itJ/5H/cjmRRj6T5mc2zgac7FUslSWcClwFnR8S/djqeLvUIMF/SPElvB5YBazocU1dT6W9t1wNPRcT/6HQ8WZJmllcDSpoKnE4L/5v0jXsZkr5DaWVBAL8CPhUROzoaVBWSfgUMRUTHu2RK+j5wNPA6pU7BF0XEts5GVSJpK3Ag8Nu062cRcVEHQwJA0oeA/wXMBHYDGyJicQfj+QDwNWAAuCEiru5ULFmSbgZOodR59Xngyoi4vqNBAZL+A/Ag8ASlf+8BvtANqyolnQCspvT/5duA2yLiqpad3wnDzMzy8CUpMzPLxQnDzMxyccIwM7NcnDDMzCwXJwwzM8vFCcPMzHJxwjAzs1ycMMzMLJf/DxD1/NJSibofAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for i in range(y_hat.shape[1]):\n",
    "plt.scatter(x_t.data.numpy(), y_hat.data.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06645858],\n",
       "       [-0.07834274],\n",
       "       [-0.09022671],\n",
       "       [-0.11399502],\n",
       "       [-0.12587917],\n",
       "       [-0.13776314],\n",
       "       [-0.14964724],\n",
       "       [-0.16153133],\n",
       "       [-0.18529958],\n",
       "       [-0.19718367],\n",
       "       [-0.2090677 ],\n",
       "       [-0.2209518 ],\n",
       "       [-0.23283589],\n",
       "       [-0.24471998],\n",
       "       [-0.25660414],\n",
       "       [-0.26848805],\n",
       "       [-0.2803722 ],\n",
       "       [-0.29225624],\n",
       "       [-0.3041404 ],\n",
       "       [-0.31602448],\n",
       "       [-0.3397926 ],\n",
       "       [-0.35167673],\n",
       "       [-0.36356083],\n",
       "       [-0.37544495],\n",
       "       [-0.38732907],\n",
       "       [-0.39921308],\n",
       "       [-0.4110972 ],\n",
       "       [-0.42298132],\n",
       "       [-0.43486533],\n",
       "       [-0.4467495 ],\n",
       "       [-0.47051764],\n",
       "       [-0.4824018 ],\n",
       "       [-0.50616986],\n",
       "       [-0.51805395],\n",
       "       [-0.5299381 ],\n",
       "       [-0.54182214],\n",
       "       [-0.5537063 ],\n",
       "       [-0.5655904 ],\n",
       "       [-0.5774744 ],\n",
       "       [-0.5893585 ],\n",
       "       [-0.60124254],\n",
       "       [-0.6131267 ],\n",
       "       [-0.6250108 ],\n",
       "       [-0.6368948 ],\n",
       "       [-0.6487789 ],\n",
       "       [-0.660663  ],\n",
       "       [-0.6725471 ],\n",
       "       [-0.6844312 ],\n",
       "       [-0.6963153 ],\n",
       "       [-0.7081994 ],\n",
       "       [-0.73196757],\n",
       "       [-0.75573575],\n",
       "       [-0.7676198 ],\n",
       "       [-0.7795039 ],\n",
       "       [-0.791388  ],\n",
       "       [-0.8032721 ],\n",
       "       [-0.8151561 ],\n",
       "       [-0.82704026],\n",
       "       [-0.8389243 ],\n",
       "       [-0.8508084 ],\n",
       "       [-0.8626925 ],\n",
       "       [-0.87457657],\n",
       "       [-0.88646066],\n",
       "       [-0.89834476],\n",
       "       [-0.92211294],\n",
       "       [-0.9339971 ],\n",
       "       [-0.9458811 ],\n",
       "       [-0.9696493 ],\n",
       "       [-0.9815334 ],\n",
       "       [-1.0053016 ]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs\n",
    "x = torch.randn((5,1))\n",
    "y = torch.randn((5,1))\n",
    "xy = torch.cat([x,y], dim=1)\n",
    "x_target = torch.randn((1,1))\n",
    "\n",
    "h_specs = [(8, torch.nn.ReLU()), (6, None)]\n",
    "g_specs = [(8, torch.nn.ReLU()), (1, None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "r_i, rr, z_mean, z_logvar, z_sample, y_target, xz = r(xy, x_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (y_all, _) in enumerate(train_loader):\n",
    "        batch_size = y_all.shape[0]\n",
    "        y_all = y_all.to(device).view(batch_size, -1, 1)\n",
    "\n",
    "        N = random.randint(1, 784)  # number of context points\n",
    "        context_idx = get_context_idx(N)\n",
    "        x_context = idx_to_x(context_idx, batch_size)\n",
    "        y_context = idx_to_y(context_idx, y_all)\n",
    "        x_all = x_grid.expand(batch_size, -1, -1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_hat, z_all, z_context = model(x_context, y_context, x_all, y_all)\n",
    "\n",
    "        loss = np_loss(y_hat, y_all, z_all, z_context)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(y_all), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader),\n",
    "                       loss.item() / len(y_all)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (y_all, _) in enumerate(test_loader):\n",
    "            y_all = y_all.to(device).view(y_all.shape[0], -1, 1)\n",
    "            batch_size = y_all.shape[0]\n",
    "\n",
    "            N = 300\n",
    "            context_idx = get_context_idx(N)\n",
    "            x_context = idx_to_x(context_idx, batch_size)\n",
    "            y_context = idx_to_y(context_idx, y_all)\n",
    "\n",
    "            y_hat, z_all, z_context = model(x_context, y_context)\n",
    "            test_loss += np_loss(y_hat, y_all, z_all, z_context).item()\n",
    "\n",
    "            if i == 0:  # save PNG of reconstructed examples\n",
    "                plot_Ns = [10, 100, 300, 784]\n",
    "                num_examples = min(batch_size, 16)\n",
    "                for N in plot_Ns:\n",
    "                    recons = []\n",
    "                    context_idx = get_context_idx(N)\n",
    "                    x_context = idx_to_x(context_idx, batch_size)\n",
    "                    y_context = idx_to_y(context_idx, y_all)\n",
    "                    for d in range(5):\n",
    "                        y_hat, _, _ = model(x_context, y_context)\n",
    "                        recons.append(y_hat[:num_examples])\n",
    "                    recons = torch.cat(recons).view(-1, 1, 28, 28).expand(-1, 3, -1, -1)\n",
    "                    background = torch.tensor([0., 0., 1.], device=device)\n",
    "                    background = background.view(1, -1, 1).expand(num_examples, 3, 784).contiguous()\n",
    "                    context_pixels = y_all[:num_examples].view(num_examples, 1, -1)[:, :, context_idx]\n",
    "                    context_pixels = context_pixels.expand(num_examples, 3, -1)\n",
    "                    background[:, :, context_idx] = context_pixels\n",
    "                    comparison = torch.cat([background.view(-1, 3, 28, 28),\n",
    "                                            recons])\n",
    "                    save_image(comparison.cpu(),\n",
    "                               'results/ep_' + str(epoch) +\n",
    "                               '_cps_' + str(N) + '.png', nrow=num_examples)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5321]], grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4119, -0.5481],\n",
      "        [ 0.4717, -0.2063],\n",
      "        [ 0.5675, -0.0696],\n",
      "        [ 0.1147, -0.1895],\n",
      "        [ 0.1097, -0.3816],\n",
      "        [ 0.0797, -0.0938],\n",
      "        [ 0.5448, -0.7040],\n",
      "        [ 0.5691, -0.2562]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.6433,  0.5584, -0.1826,  0.3319,  0.0830,  0.6283, -0.1729, -0.1951],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2440, -0.2131,  0.2332,  0.0566, -0.3294, -0.2269, -0.1447, -0.3347],\n",
      "        [ 0.0283, -0.3107,  0.2692,  0.2673,  0.0305,  0.1951,  0.1870, -0.2716],\n",
      "        [ 0.0129, -0.1644, -0.0654,  0.3496, -0.0407,  0.1357,  0.0548,  0.2479],\n",
      "        [-0.2368, -0.1613,  0.2546,  0.3408,  0.1142, -0.0618,  0.1047, -0.0254],\n",
      "        [-0.1763, -0.1979, -0.0517, -0.1633,  0.3354,  0.0125, -0.1163,  0.0073],\n",
      "        [-0.3160, -0.2325, -0.2308,  0.1812,  0.0100,  0.0083,  0.1079, -0.0380]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3353,  0.3233,  0.0661,  0.1183, -0.1239,  0.0186], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2607,  0.1865,  0.1325,  0.2106, -0.2171, -0.0595],\n",
      "        [ 0.2029, -0.1254, -0.3375, -0.2923,  0.4047,  0.0010],\n",
      "        [-0.2404, -0.2937, -0.3805,  0.1768, -0.1879, -0.2663],\n",
      "        [ 0.2868,  0.1791, -0.0951,  0.0053, -0.1278, -0.3687]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1105,  0.2548,  0.0729, -0.1114], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4028,  0.0064, -0.0249,  0.3833,  0.0162, -0.0704],\n",
      "        [ 0.3320, -0.1465, -0.1382, -0.0829,  0.0806, -0.4008],\n",
      "        [-0.1726, -0.2496, -0.2703,  0.3923,  0.3986, -0.3472],\n",
      "        [-0.2332,  0.1843, -0.3468, -0.3460, -0.0216, -0.3009]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0123, -0.2501,  0.0908, -0.2714], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2997, -0.0102,  0.4004, -0.3731,  0.3606],\n",
      "        [-0.3103, -0.1777, -0.3375,  0.1334, -0.3091],\n",
      "        [ 0.3967, -0.0635, -0.0626,  0.2890,  0.4078],\n",
      "        [-0.3859,  0.3965,  0.0443, -0.2366,  0.2719],\n",
      "        [-0.2947, -0.4359,  0.0695,  0.0807, -0.1767],\n",
      "        [-0.2343,  0.1383,  0.0889, -0.4181, -0.0469],\n",
      "        [ 0.1637,  0.2919,  0.3134, -0.0702, -0.2871],\n",
      "        [-0.4145, -0.0579,  0.3256,  0.0728, -0.0056]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0096, -0.2154, -0.2624,  0.2968,  0.1072, -0.3381, -0.2283, -0.1194],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0629, -0.2893,  0.3271, -0.0618, -0.1909, -0.3381, -0.1433, -0.1298]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3066], requires_grad=True)\n",
      "h_layer0\n",
      "h_layer0_act\n",
      "h_layer1\n",
      "r_to_mean\n",
      "r_to_logvar\n",
      "r_to_logvar_softplus\n",
      "g_layer0\n",
      "g_layer0_act\n",
      "g_layer1\n"
     ]
    }
   ],
   "source": [
    "for i in r.parameters():\n",
    "    print(i)\n",
    "\n",
    "for i in r._modules:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
